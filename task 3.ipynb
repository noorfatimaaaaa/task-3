{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E6p2OA5SKR6"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"voicegpt.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1qY4G249zGOyd3UGP3bbplXTw-rjMeBEo\n",
        "\"\"\"\n",
        "\n",
        "! pip install gradio openai gtts pydub numpy requests groq openai-whisper\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "import os #intract with the operating system\n",
        "\n",
        "import gradio as gr # Used for creating web-based user interfaces for machine learning apps\n",
        "\n",
        "import whisper # OpenAI's speech recognition model for transcribing audio to text\n",
        "\n",
        "from gtts import gTTS # Google Text-to-Speech API to convert text into spoken audio\n",
        "\n",
        "from IPython.display import Audio # For playing audio directly in Jupyter Notebooks\n",
        "\n",
        "from groq import Groq # Likely a client for interacting with Groq's AI APIs (alternative to OpenAI)\n",
        "\n",
        "import openai # OpenAI Python library to interact with GPT models and other OpenAI services\n",
        "\n",
        "import numpy as np # Numerical computing library used for manipulating audio data and arrays\n",
        "\n",
        "\n",
        "import os # Provides tools for interacting with the operating system\n",
        "\n",
        "import gradio as gr # Used to create interactive web UIs for machine learning and AI applications\n",
        "\n",
        "import whisper # OpenAIâ€™s automatic speech recognition (ASR) model to convert speech to text\n",
        "\n",
        "from gtts import gTTS # Google Text-to-Speech: converts text into spoken audio using Google's TTS engine\n",
        "\n",
        "from groq import Groq, GroqError # Groq: client for accessing Groq's AI APIs; GroqError handles API-related exceptions\n",
        "\n",
        "from typing import Tuple, Union # Provides type hints for better code clarity and editor support (e.g., function return types)\n",
        "\n",
        "# Initialize Whisper model\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Initialize Groq API client with the API key directly\n",
        "api_key = \"groq_api_key\"\n",
        "try:\n",
        "    client = Groq(api_key=api_key)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to initialize Groq client: {e}\")\n",
        "\n",
        "def transcribe_and_respond(audio: str) -> Tuple[str, Union[str, None]]:\n",
        "    try:\n",
        "        # Step 1: Transcribe the audio using Whisper\n",
        "        transcription = model.transcribe(audio)\n",
        "        user_input = transcription['text']\n",
        "\n",
        "        # Step 2: Generate a response using Groq API and LLaMA model\n",
        "        try:\n",
        "            chat_completion = client.chat.completions.create(\n",
        "                messages=[{\"role\": \"user\", \"content\": user_input}],\n",
        "                model=\"llama3-8b-8192\",\n",
        "            )\n",
        "            response_text = chat_completion.choices[0].message.content\n",
        "        except GroqError as e:\n",
        "            return f\"Error in Groq API call: {e}\", None\n",
        "\n",
        "        # Step 3: Convert the response text to speech using gTTS\n",
        "        tts = gTTS(response_text)\n",
        "        audio_path = \"response.mp3\"\n",
        "        tts.save(audio_path)\n",
        "\n",
        "        return response_text, audio_path\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        return \"Error: Audio file not found.\", None\n",
        "    except whisper.WhisperError as e:\n",
        "        return f\"Error in transcription: {e}\", None\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred: {e}\", None\n",
        "\n",
        "# Gradio interface for real-time interaction\n",
        "interface = gr.Interface(\n",
        "    fn=transcribe_and_respond,\n",
        "    inputs=gr.Audio(type=\"filepath\"),\n",
        "    outputs=[gr.Textbox(label=\"Response\"), gr.Audio(label=\"Voice Response\")],\n",
        "    live=True\n",
        ")\n",
        "\n",
        "interface.launch()"
      ]
    }
  ]
}